{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21b13d2-1aba-4e7f-9ec3-a2c459a1a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: markovify in /Users/kiran_g/Library/Python/3.9/lib/python/site-packages (0.9.4)\n",
      "Requirement already satisfied: unidecode in /Users/kiran_g/Library/Python/3.9/lib/python/site-packages (from markovify) (1.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bf4534-56d1-485a-aa26-5950637125df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5d53bb-1143-496d-ac47-245fe3173ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading CSV file\n",
    "inp = pd.read_csv('abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c2688a3-e5bd-4deb-8910-73fb16967553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the top rows \n",
    "print(\"Top three rows of the dataset:\")\n",
    "inp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e69cb5c7-63a4-426c-9acf-887957484994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three rows of the dataset:\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "\n",
      "Ten randomly generated sentences:\n",
      "wikileaks back online early\n",
      "the great northern highway roll\n",
      "nrn cwa brings christmas to be alive after 31yo female dies\n",
      "ian macfarlane hints party swap could be capped to increase\n",
      "scott driscoll pleads guilty to drug trafficking sentence in bali\n",
      "fires in south east landscapes\n",
      "lions vow to run wild in alberta; canada\n",
      "circus giant beats elephant abuse charges laid over people smuggling is finished\n",
      "a big drawcard for bush businesses\n",
      "former north coast koa\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 2: Import required libraries\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "import markovify  # Import markovify for building Markov chain models\n",
    "\n",
    "# Step 3: Load the CSV file\n",
    "inp = pd.read_csv('abcnews-date-text.csv')  # Load the CSV file into a pandas DataFrame named 'inp'\n",
    "\n",
    "# Step 4: Look at the top three rows\n",
    "print(\"Top three rows of the dataset:\")\n",
    "print(inp.head(3))  # Display the first three rows of the DataFrame\n",
    "\n",
    "# Step 5: Create a Markov chain model\n",
    "# Markov chain models can capture patterns in text data\n",
    "# Here, we create a Markov chain model using the headlines from the loaded DataFrame\n",
    "text_model = markovify.Text(inp.headline_text, state_size=2)\n",
    "# The state_size parameter determines the number of previous words to consider when generating the next word\n",
    "\n",
    "# Step 6: Generate ten randomly generated sentences\n",
    "print(\"\\nTen randomly generated sentences:\")\n",
    "# Loop to generate ten sentences\n",
    "for _ in range(10):\n",
    "    # Generate a sentence using the built model\n",
    "    sentence = text_model.make_sentence()\n",
    "    # Print the generated sentence\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8bb9a6-e5e1-43a3-bef3-b9821c387833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kiran_g/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af6f5b43-bb93-43cf-8f3d-700f72794e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mouse did not answer, so Alice went on eagerly:  `There is such a nice little dog near our house I should like to show you!\n",
      "He sent them word I had not gone (We know it to be true): If she should push the matter on, What would become of you?\n",
      "`If there's no meaning in it,' said the King, `that saves a world of trouble, you know, as we needn't try to find any.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from sumy library\n",
    "from sumy.parsers.plaintext import PlaintextParser  # To parse the text\n",
    "from sumy.nlp.tokenizers import Tokenizer  # To tokenize the text\n",
    "from sumy.summarizers.lsa import LsaSummarizer  # LSA summarizer\n",
    "\n",
    "# Path to the file containing the text to be summarized\n",
    "file_path = 'alice.txt'\n",
    "\n",
    "# Open the file and read its content\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a plaintext parser with English tokenizer\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "# Initialize the LSA summarizer\n",
    "summarizer = LsaSummarizer()\n",
    "\n",
    "# Generate a summary with 3 sentences (you can adjust this number as needed)\n",
    "summary = summarizer(parser.document, 3)\n",
    "\n",
    "# Print the generated summary\n",
    "for sentence in summary:\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1df39af-09c7-4522-8e1f-dafa60b920fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: matrix,reduction,technique,widely,data,analysis,used,learning,python,machine\n",
      "Topic 2: factorization,data,applying,widely,nmf,reduction,modeling,used,dimensionality,topic\n",
      "Topic 3: language,factorization,dimensionality,modeling,nmf,applying,text,needs,data,preprocessed\n",
      "Topic 4: dimensionality,data,applying,natural,widely,library,sklearn,python,machine,learning\n",
      "Topic 5: language,factorization,dimensionality,modeling,data,nmf,needs,preprocessed,text,applying\n",
      "Topic 6: applying,modeling,topic,machine,used,python,learning,data,analysis,widely\n",
      "Topic 7: reduction,used,method,factorization,non,negative,matrix,popular,topic,modeling\n",
      "Topic 8: language,factorization,dimensionality,modeling,data,applying,preprocessed,nmf,text,needs\n",
      "Topic 9: method,factorization,dimensionality,topic,modeling,used,technique,natural,processing,language\n",
      "Topic 10: applying,widely,modeling,topic,method,non,factorization,negative,matrix,popular\n",
      "Topic 11: factorization,data,applying,widely,topic,nmf,modeling,used,dimensionality,reduction\n",
      "Topic 12: factorization,data,applying,widely,topic,modeling,used,reduction,nmf,dimensionality\n",
      "Topic 13: factorization,dimensionality,data,applying,widely,natural,processing,language,modeling,topic\n",
      "Topic 14: applying,widely,modeling,topic,popular,matrix,negative,non,factorization,method\n",
      "Topic 15: dimensionality,data,applying,natural,widely,learning,machine,python,sklearn,library\n",
      "Topic 16: applying,widely,modeling,topic,factorization,method,popular,negative,matrix,non\n",
      "Topic 17: factorization,dimensionality,data,language,used,modeling,processing,topic,natural,technique\n",
      "Topic 18: applying,preprocessed,text,machine,learning,python,used,widely,analysis,data\n",
      "Topic 19: data,needs,applying,preprocessed,text,topic,dimensionality,reduction,modeling,nmf\n",
      "Topic 20: dimensionality,data,applying,natural,widely,learning,machine,python,sklearn,library\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For TF-IDF vectorization\n",
    "from sklearn.decomposition import NMF  # For Non-Negative Matrix Factorization\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Given text data\n",
    "text_data = [\n",
    "    \"topic modeling is a technique used in natural language processing\",\n",
    "    \"non-negative matrix factorization is a popular method for topic modeling\",\n",
    "    \"sklearn is a machine learning library in python\",\n",
    "    \"python is widely used for data analysis and machine learning\",\n",
    "    \"NMF can be used for topic modeling and dimensionality reduction\",\n",
    "    \"text data needs to be preprocessed before applying NMF\",\n",
    "]\n",
    "\n",
    "# Clean the text data by converting it to lowercase\n",
    "cleaned_text_data = [text.lower() for text in text_data]\n",
    "\n",
    "# Initialize a TF-IDF vectorizer with English stop words\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the text data into a TF-IDF matrix\n",
    "X = vectorizer.fit_transform(cleaned_text_data)\n",
    "\n",
    "# Number of topics to extract\n",
    "num_topics = 20\n",
    "\n",
    "# Initialize and fit the NMF model with the TF-IDF matrix\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_model.fit(X)\n",
    "\n",
    "# Get the feature names (terms) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the top words for each topic\n",
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    # Sort the indices of the words in the topic by their corresponding weights and get the top 10 words\n",
    "    top_words_indices = topic.argsort()[-10:]\n",
    "    # Get the feature names (words) using the indices\n",
    "    top_words = [feature_names[idx] for idx in top_words_indices]\n",
    "    # Print the topic number and its top words\n",
    "    print(\"Topic {}: {}\".format(i + 1, \",\".join(top_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155b654-31e1-41e5-ae73-ebe2eccd3e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
