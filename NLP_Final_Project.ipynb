{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765e0435-7985-4d18-9e7f-db44183d3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiran_g/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kiran_g/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/kiran_g/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/kiran_g/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiran_g/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import sys \n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from textblob import TextBlob\n",
    "import gramformer\n",
    "from gramformer import Gramformer\n",
    "gf = Gramformer(models=1, use_gpu=False)\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a90e0b-4109-44a5-95ab-3ef6d611e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kiran_g/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences extracted and saved to 'extracted_sentences.txt'\n",
      "Combined cleaned text written to: combined_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "url = \"https://en.wikipedia.org/wiki/India\" # URL of the website to scrape\n",
    "response = requests.get(url)# Fetch the webpage content\n",
    "if response.status_code == 200:# Check if the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')# Find all paragraphs\n",
    "    with open(\"extracted_sentences.txt\", \"w\") as file: # Open a text file to save the extracted sentences\n",
    "        for paragraph in paragraphs: # Loop through all paragraphs  # Get the text content of each paragraph\n",
    "            text = paragraph.get_text() # Split the text into individual sentences\n",
    "            sentences = sent_tokenize(text)  # Use NLTK to split into sentences\n",
    "            for sentence in sentences:\n",
    "                file.write(sentence.strip() + \"\\n\")\n",
    "    print(\"Sentences extracted and saved to 'extracted_sentences.txt'\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "def remove_punctuation(text):\n",
    "    tokens = word_tokenize(text) # Tokenize the text into words\n",
    "    tokens = [word for word in tokens if word.isalnum()]# Keep only alphanumeric tokens (remove punctuation)\n",
    "    cleaned_text = ' '.join(tokens)  # Join tokens back into a cleaned string\n",
    "    return cleaned_text\n",
    "    \n",
    "# Paths to existing  text files\n",
    "input_files = [\"extracted_sentences.txt\", \"chat.txt\"]  # List of input file paths\n",
    "output_file_path = \"combined_cleaned.txt\"  # Output file path\n",
    "\n",
    "cleaned_texts = []# Initialize a list to store cleaned texts\n",
    "# Read and clean each text file\n",
    "for input_file_path in input_files:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        cleaned_text = remove_punctuation(text)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "combined_cleaned_text = ' '.join(cleaned_texts)# Concatenate all cleaned texts\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:# Write the combined cleaned text to the output file\n",
    "    file.write(combined_cleaned_text)\n",
    "\n",
    "print(\"Combined cleaned text written to:\", output_file_path)# Print the final combined cleaned text or a success message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf144552-f71b-4e26-b56c-0f9d364c195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader, brown\n",
    "import os\n",
    "\n",
    "\n",
    "def get_trigram_freq(tokens):# Function to calculate trigram frequency distribution\n",
    "    tgs = list(nltk.trigrams(tokens))    # Create a list of trigrams from the given tokens\n",
    "    a, b, c = list(zip(*tgs))    # Unzip the trigrams into three separate lists\n",
    "    bgs = list(zip(a, b))# Create bigrams from the first two elements of each trigram\n",
    "    # Return a Conditional Frequency Distribution where the key is a bigram\n",
    "    # and the frequency distribution is over the third element of each trigram\n",
    "    return nltk.ConditionalFreqDist(list(zip(bgs, c)))\n",
    "\n",
    "\n",
    "def get_bigram_freq(tokens):# Function to calculate bigram frequency distribution\n",
    "    bgs = list(nltk.bigrams(tokens)) # Create a list of bigrams from the given tokens\n",
    "    return nltk.ConditionalFreqDist(bgs)# Return a Conditional Frequency Distribution of the bigrams\n",
    "\n",
    "\n",
    "def appendwithcheck(preds, to_append):# Function to append an item to a list, ensuring no duplicate keys\n",
    "    for pred in preds:# Check if the first element of 'to_append' already exists in 'preds'\n",
    "        if pred[0] == to_append[0]:\n",
    "            return  # If it exists, do nothing and return\n",
    "    preds.append(to_append)# If it doesn't exist, append 'to_append' to 'preds'\n",
    "\n",
    "\n",
    "def incomplete_pred(words, n):# Function to get predictions for incomplete words based on previous context\n",
    "    all_succeeding = bgs_freq[(words[n-2])].most_common() # Get the most common succeeding words for a given bigram\n",
    "    preds = []# Initialize a list to store predictions and a counter\n",
    "    number = 0\n",
    "    for pred in all_succeeding:# Check for words that start with the same prefix as the incomplete word\n",
    "        if pred[0].startswith(words[n-1]):# If the first character of 'pred' matches the incomplete word's first character\n",
    "            appendwithcheck(preds, pred)  # Add it to 'preds'\n",
    "            number += 1  # Increment the counter\n",
    "        if number == 3: # If we have 3 predictions, return them\n",
    "            return preds\n",
    "\n",
    "    if len(preds) < 3:# If there are fewer than 3 predictions, calculate edit distances\n",
    "        med = []# Create a list of tuples (word, edit distance from the incomplete word)\n",
    "        for pred in all_succeeding:\n",
    "            med.append((pred[0], nltk.edit_distance(pred[0], words[n-1], transpositions=True)))\n",
    "        med.sort(key=lambda x: x[1]) # Sort the list by edit distance\n",
    "\n",
    "        index = 0  # Index to iterate through 'med'\n",
    "        while len(preds) < 3:# Continue until there are at least 3 predictions\n",
    "            if index < len(med):  # Ensure we don't go out of bounds\n",
    "                if med[index][1] > 0: # If edit distance is greater than zero, append the prediction\n",
    "                    appendwithcheck(preds, med[index])\n",
    "                index += 1\n",
    "            if index >= len(med):# If the index exceeds the length of 'med', return the current predictions\n",
    "                return preds\n",
    "    \n",
    "    return preds  # Return the final list of predictions\n",
    "\n",
    "new_corpus = PlaintextCorpusReader('./', '.*')# Load a new corpus and get tokens from a specific file along with Brown corpus\n",
    "tokens = brown.words() + new_corpus.words('extracted_sentences.txt')\n",
    "\n",
    "bgs_freq = get_bigram_freq(tokens)# Generate bigram and trigram frequency distributions from the tokens\n",
    "tgs_freq = get_trigram_freq(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbeaa263-af3b-44a7-ac4a-105a688b730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob  # For text correction\n",
    "from nltk.corpus import PlaintextCorpusReader, brown\n",
    "\n",
    "def predict_page():# Function to predict the next word(s) given an input sentence\n",
    "    enter = input(\"Type sentence for predicting next word \")# Get a sentence from user input\n",
    "    # Split the input string into individual words\n",
    "    words = enter.strip().split()  # Trim and split into words\n",
    "    n = len(words)    # Get the number of words in the input\n",
    "    work = \"pred\" # Variable to control prediction mode\n",
    "    last_word = words[-1]  # Get the last word from the input\n",
    "    if last_word in bgs_freq: # Determine if there's bigram frequency data for the last word\n",
    "        if len(bgs_freq[last_word].most_common()) == 0: # If there are no common bigrams, treat it as incomplete prediction\n",
    "            inc = incomplete_pred(words, n)# Call the incomplete prediction function with current words\n",
    "            print(\"inc ->\", inc)\n",
    "            return inc  # Exit early after printing incomplete predictions\n",
    "        else:\n",
    "            work = \"pred\" # The word is recognized, continue with prediction\n",
    "    else:\n",
    "        inc = incomplete_pred(words, n)# If the last word is not in bigram frequency, treat it as incomplete\n",
    "        print(\"inc ->\", inc)\n",
    "        return inc\n",
    "    \n",
    "    if work == \"pred\":# Prediction mode for complete words\n",
    "        if n == 1:# Prediction for a single word input\n",
    "            bgs = bgs_freq[(enter.strip())].most_common(5)  # Get top 5 bigrams\n",
    "            pred = [i[0] for i in bgs]  # Extract the predicted words\n",
    "        elif n > 1:\n",
    "            tgs = tgs_freq[(words[n-2], words[n-1])].most_common(5)# Prediction for two or more words using trigrams\n",
    "            pred = [i[0] for i in tgs]  # Extract predicted words\n",
    "    print(\"Below are the top 5 predicted  words :\")# Display predictions\n",
    "    return pred  # Return the predicted words\n",
    "\n",
    "def predict_page1():# Function to correct a given sentence\n",
    "    enter = input(\"Enter sentence for correction \")# Get a sentence from user input\n",
    "     # Use TextBlob to correct the sentence\n",
    "    corrected_sentence = TextBlob(enter.strip()).correct()  # Correction via TextBlob\n",
    "    corrected_sentence_str = str(corrected_sentence) # Convert the corrected sentence to a string\n",
    "    corrections = gf.correct(corrected_sentence_str) # Perform additional corrections if needed (assuming `gf` has a `correct` function) \n",
    "    print(\"Below is the sentence after correction:\")\n",
    "    print(corrections)\n",
    "    # Perform additional corrections if needed (assuming `gf` has a `correct` function)\n",
    "    return enter , corrections# Return the corrected sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc7c5b10-6b21-44bd-b0a3-e89977965877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SENTENCE CORRECTION AND WORD GENARATOR \n",
      " \n",
      "\n",
      "1 WORD GENARATOR \n",
      "2 SENTENCE CORRECTION \n",
      " please Enter number below \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n",
      "Type sentence for predicting next word  this is not\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the top 5 predicted  words :\n",
      "['a', 'the', 'to', 'so', 'only']\n"
     ]
    }
   ],
   "source": [
    "print(\" SENTENCE CORRECTION AND WORD GENARATOR \\n \\n\")\n",
    "print(\"1 WORD GENARATOR \")\n",
    "print(\"2 SENTENCE CORRECTION \")\n",
    "print(\" please Enter number below \")\n",
    "input_ = int(input())\n",
    "if input_ == 1:\n",
    "    print(predict_page())\n",
    "elif input_ == 2:\n",
    "    sentence, Correction  = predict_page1()\n",
    "else:\n",
    "    print(\"Enter Correct input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1105153a-03a1-4f7c-9517-57bcde6abf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: The cafe was full of custmors but the servce was too slow.\n",
      "Corrected sentence: The cafe was full of customs but the service was too slow.\n",
      "Reference sentence: The café was full of customers, but the service was too slow.\n",
      "BLEU score: 0.4366835442847812\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Original sentence with errors\n",
    "input_sentence = \"The cafe was full of custmors but the servce was too slow.\"\n",
    "\n",
    "# Correct the sentence with TextBlob\n",
    "corrected_sentence = TextBlob(input_sentence).correct()  # Correcting the original sentence\n",
    "corrected_sentence_str = str(corrected_sentence) # Convert the corrected sentence to a string\n",
    "corrections = gf.correct(corrected_sentence_str)\n",
    "corrected_tokens = []\n",
    "for sentence in corrections:\n",
    "    corrected_tokens.extend(sentence.split())\n",
    "    \n",
    "# print(type(corrections))\n",
    "# # Convert corrected sentence to tokenized list\n",
    "# corrected_tokens = list(corrections.words)\n",
    "# Reference sentence (expected correction)\n",
    "reference_sentence = \"The café was full of customers, but the service was too slow.\"\n",
    "\n",
    "# Tokenize the reference sentence for BLEU calculation\n",
    "reference_tokens = list(TextBlob(reference_sentence).words)\n",
    "\n",
    "# Calculate the BLEU score\n",
    "smoothing = SmoothingFunction()\n",
    "\n",
    "bleu_score = sentence_bleu(\n",
    "    [reference_tokens],  # The correct sentence(s) to compare against\n",
    "    corrected_tokens,  # The corrected output\n",
    "    smoothing_function=smoothing.method1,  # Smoothing to prevent zero scores\n",
    "    weights=(0.25, 0.25, 0.25, 0.25)  # Default BLEU weights for 1-grams to 4-grams\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Original sentence:\", input_sentence)\n",
    "print(\"Corrected sentence:\", corrected_sentence)\n",
    "print(\"Reference sentence:\", reference_sentence)\n",
    "print(\"BLEU score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce92c0-a917-4fbb-a48f-d3888c24a88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
